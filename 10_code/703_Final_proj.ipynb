{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s0emZpygcXY"
      },
      "source": [
        "# System Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkkH2Mc7Y2ag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cedeccd-e3f8-4ec8-d5ed-86c7232810f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for re\u001b[0m\n",
            "Collecting lyricsgenius\n",
            "  Downloading lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from lyricsgenius) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from lyricsgenius) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (2021.10.8)\n",
            "Installing collected packages: lyricsgenius\n",
            "Successfully installed lyricsgenius-3.0.1\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install re \n",
        "!pip install lyricsgenius\n",
        "!pip install keras\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XLvq5YsZoFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24822791-d1e4-4009-a3d2-e5ff48f1cc12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ],
      "source": [
        "import lyricsgenius as lg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Activation, Dropout\n",
        "import numpy as np \n",
        "#from tensorflow import keras\n",
        "#from __future__ import print_function\n",
        "\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "#from keras.utils.data_utils import get_file\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxt1gNHxfZee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202b6132-687b-42ad-fd2a-bc12934ac9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLhoivMiZaiK"
      },
      "source": [
        "# Getting the lyrics from Genius API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the API times out before you are able to get 200 songs, skip to the section titled \"Testing and Training Split\", and get the data from Github."
      ],
      "metadata": {
        "id": "7xqCJvGeTax2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Z7M_-55qzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77e7402-49c7-43af-a596-7c6421c32805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for songs by Taylor Swift...\n",
            "\n",
            "Song 1: \"​cardigan\"\n",
            "Song 2: \"All Too Well (10 Minute Version) (Taylor’s Version) [From the Vault]\"\n",
            "Song 3: \"​exile\"\n",
            "Song 4: \"Lover\"\n",
            "Song 5: \"​the 1\"\n",
            "Song 6: \"Look What You Made Me Do\"\n",
            "Song 7: \"​betty\"\n",
            "Song 8: \"​august\"\n",
            "Song 9: \"End Game\"\n",
            "Song 10: \"You Need To Calm Down\"\n",
            "Song 11: \"Blank Space\"\n",
            "Song 12: \"​champagne problems\"\n",
            "Song 13: \"ME!\"\n",
            "Song 14: \"...Ready for It?\"\n",
            "Song 15: \"​my tears ricochet\"\n",
            "Song 16: \"​willow\"\n",
            "Song 17: \"​invisible string\"\n",
            "Song 18: \"Cruel Summer\"\n",
            "Song 19: \"​the last great american dynasty\"\n",
            "Song 20: \"All Too Well\"\n",
            "Song 21: \"​seven\"\n",
            "Song 22: \"Delicate\"\n",
            "Song 23: \"Call It What You Want\"\n",
            "Song 24: \"Style\"\n",
            "Song 25: \"​illicit affairs\"\n",
            "Song 26: \"​this is me trying\"\n",
            "\"Bad Blood (Remix)\" is not valid. Skipping.\n",
            "\"I Don’t Wanna Live Forever\" is not valid. Skipping.\n",
            "Song 27: \"Love Story\"\n",
            "Song 28: \"​evermore\"\n",
            "Song 29: \"​happiness\"\n",
            "Song 30: \"​peace\"\n",
            "Song 31: \"Gorgeous\"\n",
            "\"All Too Well (10 Minute Version) (Taylor’s Version) [Live Acoustic]\" is not valid. Skipping.\n",
            "Song 32: \"​tolerate it\"\n",
            "Song 33: \"​mirrorball\"\n",
            "Song 34: \"​mad woman\"\n",
            "Song 35: \"​ivy\"\n",
            "Song 36: \"The Man\"\n",
            "Song 37: \"Don’t Blame Me\"\n",
            "Song 38: \"Dress\"\n",
            "Song 39: \"I Did Something Bad\"\n",
            "Song 40: \"​no body, no crime\"\n",
            "Song 41: \"​gold rush\"\n",
            "Song 42: \"Getaway Car\"\n",
            "Song 43: \"​hoax\"\n",
            "Song 44: \"​epiphany\"\n",
            "Song 45: \"Wildest Dreams\"\n",
            "Song 46: \"The Archer\"\n",
            "Song 47: \"Mr. Perfectly Fine (Taylor’s Version) [From the Vault]\"\n",
            "Song 48: \"London Boy\"\n",
            "Song 49: \"Cornelia Street\"\n",
            "Song 50: \"Miss Americana & The Heartbreak Prince\"\n",
            "Song 51: \"​’tis the damn season\"\n",
            "Song 52: \"Bad Blood\"\n",
            "Song 53: \"Daylight\"\n",
            "Song 54: \"​the lakes\"\n",
            "Song 55: \"King of My Heart\"\n",
            "Song 56: \"Death by a Thousand Cuts\"\n",
            "Song 57: \"New Year’s Day\"\n",
            "Song 58: \"I Forgot That You Existed\"\n",
            "Song 59: \"​coney island\"\n",
            "Song 60: \"Paper Rings\"\n",
            "Song 61: \"​cowboy like me\"\n",
            "Song 62: \"So It Goes...\"\n",
            "Song 63: \"False God\"\n",
            "Song 64: \"This Is Why We Can’t Have Nice Things\"\n",
            "Song 65: \"​dorothea\"\n",
            "Song 66: \"Afterglow\"\n",
            "Song 67: \"Dancing with Our Hands Tied\"\n",
            "Song 68: \"I Think He Knows\"\n",
            "Song 69: \"​l​ong story short\"\n",
            "Song 70: \"Soon You’ll Get Better\"\n",
            "Song 71: \"You Belong with Me\"\n",
            "Song 72: \"​marjorie\"\n",
            "Song 73: \"Clean\"\n",
            "Song 74: \"​closure\"\n",
            "Song 75: \"Nothing New (Taylor’s Version) [From the Vault]\"\n",
            "Song 76: \"Out of the Woods\"\n",
            "Song 77: \"Shake It Off\"\n",
            "Song 78: \"​r​ight where you left me\"\n",
            "Song 79: \"It’s Nice to Have a Friend\"\n",
            "\"Why She Disappeared [Poem]\" is not valid. Skipping.\n",
            "Song 80: \"Holy Ground\"\n",
            "Song 81: \"I Knew You Were Trouble.\"\n",
            "Song 82: \"We Are Never Ever Getting Back Together\"\n",
            "Song 83: \"​it’s time to go\"\n",
            "Song 84: \"22\"\n",
            "Song 85: \"Begin Again\"\n",
            "Song 86: \"New Romantics\"\n",
            "Song 87: \"I Bet You Think About Me (Taylor’s Version) [From the Vault]\"\n",
            "Song 88: \"Dear John\"\n",
            "Song 89: \"You All Over Me (Taylor’s Version) [From the Vault]\"\n",
            "Song 90: \"State of Grace\"\n",
            "Song 91: \"Enchanted\"\n",
            "Song 92: \"Wonderland\"\n",
            "Song 93: \"You Are in Love\"\n",
            "Song 94: \"This Love\"\n",
            "Song 95: \"Everything Has Changed\"\n",
            "\"Lover (Remix)\" is not valid. Skipping.\n",
            "Song 96: \"Red\"\n",
            "Song 97: \"Better Man (Taylor’s Version) [From the Vault]\"\n",
            "Song 98: \"I Know Places\"\n",
            "Song 99: \"Safe & Sound\"\n",
            "\"Long Live\" is not valid. Skipping.\n",
            "Song 100: \"All You Had to Do Was Stay\"\n",
            "Song 101: \"Love Story (Taylor’s Version)\"\n",
            "Song 102: \"Forever Winter (Taylor’s Version) [From the Vault]\"\n",
            "Song 103: \"Treacherous\"\n",
            "Song 104: \"Last Kiss\"\n",
            "Song 105: \"Welcome to New York\"\n",
            "Song 106: \"I Wish You Would\"\n",
            "Song 107: \"Mine\"\n",
            "Song 108: \"Run (Taylor’s Version) [From the Vault]\"\n",
            "\"If You’re Anything Like Me [Poem]\" is not valid. Skipping.\n",
            "Song 109: \"Message In A Bottle (Taylor’s Version) [From The Vault]\"\n",
            "Song 110: \"Reputation [Prologue]\"\n",
            "Song 111: \"Better Than Revenge\"\n",
            "Song 112: \"Only The Young\"\n",
            "Song 113: \"How You Get the Girl\"\n",
            "Song 114: \"The Moment I Knew\"\n",
            "Song 115: \"Back to December\"\n",
            "Song 116: \"The Lucky One\"\n",
            "Song 117: \"The Last Time\"\n",
            "Song 118: \"I Almost Do\"\n",
            "Song 119: \"Sad Beautiful Tragic\"\n",
            "Song 120: \"Mean\"\n",
            "Song 121: \"Fifteen\"\n",
            "Song 122: \"Fearless\"\n",
            "Song 123: \"That’s When (Taylor’s Version) [From the Vault]\"\n",
            "Song 124: \"Sparks Fly\"\n",
            "Song 125: \"Teardrops On My Guitar\"\n",
            "Song 126: \"Innocent\"\n",
            "Song 127: \"Picture to Burn\"\n",
            "Song 128: \"Don’t You (Taylor’s Version) [From the Vault]\"\n",
            "Song 129: \"The Story of Us\"\n",
            "Song 130: \"We Were Happy (Taylor’s Version) [From the Vault]\"\n",
            "Song 131: \"The Way I Loved You\"\n",
            "Song 132: \"Come Back... Be Here\"\n",
            "Song 133: \"Starlight\"\n",
            "Song 134: \"White Horse\"\n",
            "Song 135: \"Beautiful Ghosts\"\n",
            "Song 136: \"Haunted\"\n",
            "Song 137: \"Our Song\"\n",
            "Song 138: \"Bye Bye Baby (Taylor’s Version) [From the Vault]\"\n",
            "Song 139: \"The Very First Night (Taylor’s Version) [From the Vault]\"\n",
            "Song 140: \"Christmas Tree Farm\"\n",
            "Song 141: \"Stay Stay Stay\"\n",
            "Song 142: \"Forever & Always\"\n",
            "Song 143: \"Red (Taylor’s Version)\"\n",
            "Song 144: \"Babe (Taylor’s Version) [From the Vault]\"\n",
            "Song 145: \"Breathe\"\n",
            "Song 146: \"Tim McGraw\"\n",
            "Song 147: \"Red (Original Demo Recording)\"\n",
            "Song 148: \"Ours\"\n",
            "Song 149: \"Speak Now\"\n",
            "Song 150: \"Never Grow Up\"\n",
            "Song 151: \"I Knew You Were Trouble (Intro)\"\n",
            "Song 152: \"Ronan\"\n",
            "Song 153: \"Hey Stephen\"\n",
            "Song 154: \"The Best Day\"\n",
            "Song 155: \"Should’ve Said No\"\n",
            "Song 156: \"Girl at Home\"\n",
            "Song 157: \"You’re Not Sorry\"\n",
            "Song 158: \"Untouchable\"\n",
            "Song 159: \"If This Was a Movie\"\n",
            "Song 160: \"Let’s Go (Battle)\"\n",
            "Song 161: \"Cold as You\"\n",
            "\"Reputation Magazine Vol. 1\" is not valid. Skipping.\n",
            "\"Back to December / Apologize / You’re Not Sorry (Live/2011)\" is not valid. Skipping.\n",
            "Song 162: \"The Other Side of the Door\"\n",
            "Song 163: \"September\"\n",
            "Song 164: \"Change\"\n",
            "Song 165: \"Tied Together with a Smile\"\n",
            "\"Gasoline (Remix)\" is not valid. Skipping.\n",
            "Song 166: \"Mary’s Song (Oh My My My)\"\n",
            "Song 167: \"I Knew You Were Trouble (Taylor’s Version)\"\n",
            "Song 168: \"Unreleased Songs [Discography List]\"\n",
            "Song 169: \"All Too Well (Taylor’s Version)\"\n",
            "Song 170: \"Crazier\"\n",
            "Song 171: \"Tell Me Why\"\n",
            "Song 172: \"Fearless (Taylor’s Version)\"\n",
            "Song 173: \"Jump Then Fall\"\n",
            "Song 174: \"Superman\"\n",
            "Song 175: \"I’m Only Me When I’m with You\"\n",
            "\"Red [Liner Notes]\" is not valid. Skipping.\n",
            "Song 176: \"Stay Beautiful\"\n",
            "\"MTV VMAs 2015: Vanguard Acceptance Speech\" is not valid. Skipping.\n",
            "\"A Message From Taylor\" is not valid. Skipping.\n",
            "Song 177: \"State of Grace (Taylor’s Version)\"\n",
            "\"I Know Places (Voice Memo)\" is not valid. Skipping.\n",
            "\"Speak Now [Liner Notes]\" is not valid. Skipping.\n",
            "Song 178: \"The Outside\"\n",
            "\"1989 [Liner Notes]\" is not valid. Skipping.\n",
            "Song 179: \"SuperStar\"\n",
            "\"Blank Space (Voice Memo)\" is not valid. Skipping.\n",
            "Song 180: \"You Belong With Me (Taylor’s Version)\"\n",
            "Song 181: \"The Moment I Knew (Taylor’s Version)\"\n",
            "Song 182: \"Come In With the Rain\"\n",
            "Song 183: \"Ronan (Taylor’s Version)\"\n",
            "Song 184: \"Invisible\"\n",
            "Song 185: \"The Way I Loved You (Taylor’s Version)\"\n",
            "Song 186: \"A Place In This World\"\n",
            "Song 187: \"Treacherous (Taylor’s Version)\"\n",
            "Song 188: \"Wildest Dreams (Taylor’s Version)\"\n",
            "\"Red (Live cover)\" is not valid. Skipping.\n",
            "Song 189: \"Girl At Home (Taylor’s Version)\"\n",
            "Song 190: \"Come Back...Be Here (Taylor’s Version)\"\n",
            "Song 191: \"Holy Ground (Taylor’s Version)\"\n",
            "Song 192: \"Begin Again (Taylor’s Version)\"\n",
            "Song 193: \"Sparks Fly (Original Version)\"\n",
            "Song 194: \"Macavity\"\n",
            "Song 195: \"Hey Stephen (Taylor’s Version)\"\n",
            "Song 196: \"The Last Time (Taylor’s Version)\"\n",
            "Song 197: \"I Almost Do (Taylor’s Version)\"\n",
            "Song 198: \"Fifteen (Taylor’s Version)\"\n",
            "Song 199: \"Sad Beautiful Tragic (Taylor’s Version)\"\n",
            "Song 200: \"The Other Side of the Door (Taylor’s Version)\"\n",
            "\n",
            "Reached user-specified song limit (200).\n",
            "Done. Found 200 songs.\n"
          ]
        }
      ],
      "source": [
        "token = 'AfdLFfPg5oAQFc-mF6EcotZXRkH_TFBRE9eDdbQ5RkZwtBuVGXJyBlfDrkiCByBt'\n",
        "artists = []\n",
        "genius = lg.Genius(token, skip_non_songs=True, excluded_terms=[\"(Remix)\", \"(Live)\",\"Speech\",\"Tour\",\"Poem\",\"(Voice Memo)\", \"Magazine\", \"A Message From Taylor\"],remove_section_headers = True) # token\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        artists.append(genius.search_artist(\"Taylor Swift\", max_songs=200,sort=\"popularity\"))\n",
        "        break\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Xk6yNgx7biay"
      },
      "outputs": [],
      "source": [
        "cList = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",\"you've\": \"you have\", \"'cause\":'because'}\n",
        "c_re = re.compile('(%s)' % '|'.join(cList.keys())) #regular expression object, with format of contractions like i'm, i've\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return cList[match.group(0)] \n",
        "    return c_re.sub(replace, text)  \n",
        "\n",
        "#### removing stopwords and non-lyric stuff\n",
        "songs = artists[0].songs\n",
        "lyrics_wo_stopword=[]\n",
        "for i in songs:\n",
        "    lyric = expandContractions(i.lyrics.lower()) # expanding abbreviation to two words format\n",
        "    lyric  = re.sub(r\"\\W+\",\" \",lyric) \n",
        "    lyric  = re.sub(r\"[a-z]*\\d+embedshare\",\" \",lyric)\n",
        "    lyric  = re.sub(r\"urlcopyembedcopy\",\" \",lyric)\n",
        "    lyric_token = word_tokenize(lyric)\n",
        "    filtered_words = [word.lower() for word in lyric_token  if word.lower() not in stopwords.words('english')]\n",
        "    lyrics_wo_stopword.append(' '.join(filtered_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCAvVOWagVr1"
      },
      "source": [
        "# Assigning Sentiment Tags to Lyrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NK5oPmwMfj8E"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "lyric_df = pd.DataFrame(lyrics_wo_stopword,columns =['lyrics'])\n",
        "lyric_df['pos']  = lyric_df['lyrics'].apply(lambda x: sia.polarity_scores(x)['pos'])\n",
        "lyric_df['neg']  = lyric_df['lyrics'].apply(lambda x: sia.polarity_scores(x)['neg'])\n",
        "lyric_df['compound']  = lyric_df['lyrics'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "lyric_df['sentiment'] = lyric_df['compound'].apply(lambda x: 'pos' if (x>0) else 'neg') #using compound score, if >0 then assign it as positive text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37HCv__df69w",
        "outputId": "89dde26f-bbc2-4278-b36d-d0f8cdf428ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pos    153\n",
              "neg     47\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "lyric_df.sentiment.value_counts() # for now we have 101 positive tagged lyrics "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check if we exclude all the none lyrics stuff \n",
        "# by looking at the length of the lyrics \n",
        "total_word_count = lyric_df['lyrics'].str.split().apply(len)\n",
        "for index, value in total_word_count.items():\n",
        "    print(str(index) +':' + str(value) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73pS6jbzuKRf",
        "outputId": "4bc9e338-d606-449c-e2fb-ceacd12dfc63"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:160\n",
            "1:388\n",
            "2:219\n",
            "3:110\n",
            "4:143\n",
            "5:217\n",
            "6:177\n",
            "7:172\n",
            "8:368\n",
            "9:187\n",
            "10:264\n",
            "11:139\n",
            "12:248\n",
            "13:198\n",
            "14:121\n",
            "15:190\n",
            "16:160\n",
            "17:224\n",
            "18:170\n",
            "19:189\n",
            "20:97\n",
            "21:161\n",
            "22:245\n",
            "23:201\n",
            "24:121\n",
            "25:93\n",
            "26:168\n",
            "27:144\n",
            "28:167\n",
            "29:135\n",
            "30:180\n",
            "31:133\n",
            "32:115\n",
            "33:153\n",
            "34:172\n",
            "35:139\n",
            "36:222\n",
            "37:257\n",
            "38:298\n",
            "39:133\n",
            "40:188\n",
            "41:273\n",
            "42:107\n",
            "43:75\n",
            "44:176\n",
            "45:132\n",
            "46:225\n",
            "47:226\n",
            "48:210\n",
            "49:263\n",
            "50:180\n",
            "51:232\n",
            "52:239\n",
            "53:109\n",
            "54:205\n",
            "55:176\n",
            "56:153\n",
            "57:123\n",
            "58:164\n",
            "59:260\n",
            "60:169\n",
            "61:157\n",
            "62:150\n",
            "63:152\n",
            "64:146\n",
            "65:161\n",
            "66:218\n",
            "67:211\n",
            "68:184\n",
            "69:119\n",
            "70:151\n",
            "71:149\n",
            "72:198\n",
            "73:107\n",
            "74:157\n",
            "75:247\n",
            "76:314\n",
            "77:198\n",
            "78:94\n",
            "79:130\n",
            "80:184\n",
            "81:242\n",
            "82:174\n",
            "83:216\n",
            "84:145\n",
            "85:254\n",
            "86:177\n",
            "87:172\n",
            "88:123\n",
            "89:123\n",
            "90:184\n",
            "91:255\n",
            "92:193\n",
            "93:212\n",
            "94:218\n",
            "95:193\n",
            "96:188\n",
            "97:132\n",
            "98:80\n",
            "99:164\n",
            "100:168\n",
            "101:156\n",
            "102:118\n",
            "103:170\n",
            "104:192\n",
            "105:262\n",
            "106:187\n",
            "107:179\n",
            "108:148\n",
            "109:236\n",
            "110:220\n",
            "111:128\n",
            "112:205\n",
            "113:151\n",
            "114:185\n",
            "115:158\n",
            "116:188\n",
            "117:118\n",
            "118:100\n",
            "119:180\n",
            "120:193\n",
            "121:138\n",
            "122:101\n",
            "123:171\n",
            "124:113\n",
            "125:133\n",
            "126:147\n",
            "127:81\n",
            "128:199\n",
            "129:116\n",
            "130:150\n",
            "131:148\n",
            "132:183\n",
            "133:119\n",
            "134:136\n",
            "135:155\n",
            "136:176\n",
            "137:146\n",
            "138:165\n",
            "139:167\n",
            "140:159\n",
            "141:162\n",
            "142:194\n",
            "143:164\n",
            "144:146\n",
            "145:182\n",
            "146:156\n",
            "147:138\n",
            "148:186\n",
            "149:221\n",
            "150:62\n",
            "151:141\n",
            "152:186\n",
            "153:152\n",
            "154:129\n",
            "155:202\n",
            "156:111\n",
            "157:184\n",
            "158:194\n",
            "159:207\n",
            "160:103\n",
            "161:153\n",
            "162:133\n",
            "163:122\n",
            "164:117\n",
            "165:146\n",
            "166:186\n",
            "167:380\n",
            "168:190\n",
            "169:108\n",
            "170:133\n",
            "171:142\n",
            "172:178\n",
            "173:187\n",
            "174:114\n",
            "175:149\n",
            "176:237\n",
            "177:101\n",
            "178:132\n",
            "179:152\n",
            "180:151\n",
            "181:99\n",
            "182:140\n",
            "183:111\n",
            "184:158\n",
            "185:94\n",
            "186:116\n",
            "187:179\n",
            "188:189\n",
            "189:160\n",
            "190:128\n",
            "191:145\n",
            "192:172\n",
            "193:189\n",
            "194:205\n",
            "195:187\n",
            "196:118\n",
            "197:194\n",
            "198:105\n",
            "199:154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUIv5Ussgbhn"
      },
      "source": [
        "# Testing and Training Split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment line below if API keeps timing out (fetched from the results using the same script)\n",
        "# lyric_df = pd.read_csv('Taylor_200_song_lyrics_with_sentiment.csv', header = 0)"
      ],
      "metadata": {
        "id": "WoN8s-6pBT2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG9M9_K8goBd"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(lyric_df, test_size=0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnYvC9v5gtbA",
        "outputId": "0e34a4a2-aee5-4abe-bbd7-c039695f1832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 5)\n",
            "(40, 5)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMn_fNS9c9kO"
      },
      "source": [
        "# Naive Bayes Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqsk0ri8dA6B"
      },
      "outputs": [],
      "source": [
        "def get_text(df,  tag):\n",
        "  # Join together the text in the reviews for a particular class.\n",
        "    t = df.loc[df['sentiment'] == tag, ['lyrics']]\n",
        "    return ' '.join(t.lyrics)\n",
        "\n",
        "def count_text(text):\n",
        "  # Count up the occurence of each word.\n",
        "    lyric_token = word_tokenize(text)\n",
        "    return Counter(lyric_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAtoUoT2eOIz",
        "outputId": "88553588-39c1-45b0-b39e-e9b0ba1ce63b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3195"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_text_neg = get_text(train, 'neg')\n",
        "neg_word_cnt = count_text(train_text_neg)\n",
        "train_text_pos = get_text(train, 'pos')\n",
        "pos_word_cnt = count_text(train_text_pos)\n",
        "voc_word_cnt = neg_word_cnt + pos_word_cnt \n",
        "len(set(voc_word_cnt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z5ed7kJeWLD"
      },
      "outputs": [],
      "source": [
        "def train_naive_bayes( tag, word_cnt, voc_word_cnt):\n",
        "    \"\"\"\n",
        "    Get the log likelihood of a word existing in a certain class\n",
        "    Use log to avoid underflow\n",
        "    Input: \n",
        "        1.tag: the sentiment tag, 'pos' or 'neg'\n",
        "        2.word_cnt: the word count frequency for each class\n",
        "        3.voc_word_cnt: the word count frequency for all the training documents\n",
        "    Output: \n",
        "        1.prior: the probability of getting a class from the document \n",
        "        2.prob_dict: the log likelihood of seeing this word in a particular class\n",
        "    \"\"\"\n",
        "    prior = np.log(len(train.loc[train['sentiment'] == tag,]) / len(train))\n",
        "    prob_dict = dict()\n",
        "    for word in voc_word_cnt: \n",
        "        prob = np.log((word_cnt.get(word,0) +1 ) / (sum(word_cnt.values())+ len(voc_word_cnt))) \n",
        "        prob_dict[word] = prob\n",
        "    return prior, prob_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXbuLuSXeXa5"
      },
      "outputs": [],
      "source": [
        "def make_class_prediction(text, voc,  prior, prob):\n",
        "    \"\"\"\n",
        "    Given the trained data, predict whether the new text is positive or negative \n",
        "    Input:\n",
        "        1. text: new text that we want to predict\n",
        "        2. voc: the vocabulary list from training data set. If the word in text is not in the training set, ignore it\n",
        "        3. prior: the probability of getting a class from the document using the training data \n",
        "        4. prob: the probability of a word appearing in a specific class using the training data\n",
        "    Output:\n",
        "        1. prediction: the probability of the text being in a specific class\n",
        "    \"\"\"\n",
        "    prediction = prior\n",
        "    text_counts = Counter(count_text(text))\n",
        "    for word in text_counts:\n",
        "        if word in voc.keys():\n",
        "            p = prob[word]\n",
        "            prediction +=  np.log(text_counts.get(word)) + p\n",
        "    return prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXDFK4DreZDf"
      },
      "outputs": [],
      "source": [
        "testing_bayes = test\n",
        "# test it with the first testing set \n",
        "testing_bayes['pos_pred'] = testing_bayes['lyrics'].apply(lambda x: make_class_prediction(x, voc_word_cnt \n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[1]))\n",
        "\n",
        "testing_bayes['neg_pred'] = testing_bayes['lyrics'].apply(lambda x: make_class_prediction( x, voc_word_cnt\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j21K_FPHeik"
      },
      "outputs": [],
      "source": [
        "# assign predicted label by taking argmax of the two predictions\n",
        "testing_bayes['pred_label'] = np.where(testing_bayes['pos_pred']>testing_bayes['neg_pred'], \n",
        "                                           'pos', 'neg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN89ToPOJAsm",
        "outputId": "18c6c12b-a674-4365-a733-efa83e7840fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# testing accuracy\n",
        "sum(testing_bayes['pred_label']==testing_bayes['sentiment'])/len(testing_bayes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yrXFL6niGCr"
      },
      "outputs": [],
      "source": [
        "training_bayes = train\n",
        "# train it with the first training set \n",
        "training_bayes['pos_pred'] = training_bayes['lyrics'].apply(lambda x: make_class_prediction( x , voc_word_cnt\n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[1]))\n",
        "\n",
        "training_bayes['neg_pred'] = training_bayes['lyrics'].apply(lambda x: make_class_prediction( x, voc_word_cnt\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MaWNYfPiOnd"
      },
      "outputs": [],
      "source": [
        "# assign predicted label by taking argmax of the two predictions\n",
        "training_bayes['pred_label'] = np.where(training_bayes['pos_pred']>training_bayes['neg_pred'], \n",
        "                                           'pos', 'neg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_wdMVNiY6_",
        "outputId": "3a7c0803-2c1f-4b1b-f942-325aa9b1c650"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# training accuracy\n",
        "sum(training_bayes['pred_label']==training_bayes['sentiment'])/len(training_bayes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PhUDpdZt-Q-"
      },
      "source": [
        "## Generate Synthetic Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we want to generate synthetic data from the trained probabilities in Naive Bayes Section\n",
        "\n"
      ],
      "metadata": {
        "id": "kT_njKvr51SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the probability to ensure they sum up to 1 \n",
        "def normalize(probs):\n",
        "    prob_factor = 1 / sum(probs)\n",
        "    return [prob_factor * p for p in probs]"
      ],
      "metadata": {
        "id": "rUJmNv-Y2mPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_word_count = lyric_df['lyrics'].str.split().apply(len)\n",
        "mu = total_word_count.mean() # get the mean of lyrics length \n",
        "sigma = np.std(total_word_count) # get the standard deviation of lyrics length \n",
        "print(mu, sigma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5WdaIFnsnVv",
        "outputId": "c6706228-a83a-4793-c712-0a77f40bfc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169.77 51.378663859621724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcPvUGu-iVgg"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data(tag, n, mu, sigma, word_prob):\n",
        "    \"\"\"\n",
        "    Generating the synthtic data using the distribution of the true lyrics set \n",
        "    Input: \n",
        "        1. tag: positive class of negative class \n",
        "        2. n : number of documents you want to generate for each tag\n",
        "        3. mu: the mean of distribution of the true lyrics set \n",
        "        4. sigma: the stdev of distribution of the \n",
        "        5. word_prob: the probability of each word in a certain class from the training dataset \n",
        "    Output: \n",
        "        1. a dictionary with tag and the list of sample words\n",
        "    \"\"\"\n",
        "    syn_data = []\n",
        "    document_length_sampler = [round(num, 0) for num in np.random.normal(mu, sigma, n).tolist()]\n",
        "    for i in range(n): \n",
        "        sample_length= int(document_length_sampler[i])\n",
        "        # sample with replacement \n",
        "        sample_pos_doc = np.random.choice(list(word_prob.keys()), sample_length\n",
        "                                      , p=normalize(list(word_prob.values()))).tolist()\n",
        "        syn_data.append(' '.join(sample_pos_doc))\n",
        "    return {'lyrics':syn_data, 'sentiment':[tag]*n}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exponentiate each probability we trained for naive bayes cuz we use log to avoid underflow \n",
        "# each word probability dictionary itself should already sum up to 1 \n",
        "pos_word_prob ={k: np.exp(v) for k, v in train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[1].items()}\n",
        "neg_word_prob ={k: np.exp(v) for k, v in train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[1].items()}"
      ],
      "metadata": {
        "id": "c7vpuCSR1cXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_syn= generate_synthetic_data('pos', 200, mu, sigma, pos_word_prob);\n",
        "neg_syn= generate_synthetic_data('neg', 200, mu, sigma, neg_word_prob)"
      ],
      "metadata": {
        "id": "1UM4UQDb085k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data  = pd.concat([pd.DataFrame(pos_syn), pd.DataFrame(neg_syn)])\n",
        "syn_data = shuffle(syn_data) # shuffle the order to mix up positive data and negative dta "
      ],
      "metadata": {
        "id": "_Nbfftj44nFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test it with the first testing set \n",
        "syn_data['pos_pred'] = syn_data['lyrics'].apply(lambda x: make_class_prediction(x, voc_word_cnt \n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('pos',pos_word_cnt, voc_word_cnt)[1]))\n",
        "\n",
        "syn_data['neg_pred'] = syn_data['lyrics'].apply(lambda x: make_class_prediction( x, voc_word_cnt\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[0]\n",
        "                    , train_naive_bayes('neg',neg_word_cnt, voc_word_cnt)[1]))"
      ],
      "metadata": {
        "id": "tIfvDXCbzgRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign predicted label by taking argmax of the two predictions\n",
        "syn_data['pred_label'] = np.where(syn_data['pos_pred']>syn_data['neg_pred'], \n",
        "                                           'pos', 'neg')"
      ],
      "metadata": {
        "id": "wpKzqdqwFv2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing accuracy for synthetic data \n",
        "sum(syn_data['pred_label']==syn_data['sentiment'])/len(syn_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBc7p0UMF0WR",
        "outputId": "cf3231a6-0a9b-4ba8-d95f-bc304ac6b6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YErNhBent59"
      },
      "source": [
        "#LSTM Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training LSTM Model"
      ],
      "metadata": {
        "id": "LN4f3NL8Jh6u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY9ImP7lLA-5"
      },
      "outputs": [],
      "source": [
        "total_word_count = lyric_df['lyrics'].str.split().apply(len)\n",
        "max_features = max(total_word_count)\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(lyric_df['lyrics'].values)\n",
        "X = tokenizer.texts_to_sequences(lyric_df['lyrics'].values)\n",
        "X = pad_sequences(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9zxafX4fko",
        "outputId": "036eceba-2c9b-4a0f-bdba-79464a228ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ..., 361, 118,  23],\n",
              "       [  0,   0,   0, ..., 363, 226,  42],\n",
              "       [  0,   0,   0, ...,   4, 195,  27],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  29, 290, 121],\n",
              "       [  0,   0,   0, ..., 230,  68,   5],\n",
              "       [  0,   0,   0, ..., 127,  46,  85]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUnABtMNRtQz",
        "outputId": "9148796b-4718-49d5-a136-ef3764054d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 277, 64)           24832     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 16)                5184      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,033\n",
            "Trainable params: 30,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 64, input_length=X.shape[1]))\n",
        "model.add(LSTM(16))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_KTfNJdR4XT",
        "outputId": "4319600e-1f24-4c12-dd7f-9c9b9d833221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 277) (160,)\n",
            "(40, 277) (40,)\n"
          ]
        }
      ],
      "source": [
        "lyric_df['sentiment_bin'] = lyric_df['compound'].apply(lambda x: 1 if (x>0) else 0) #using compound score, if >0 then assign it as positive text\n",
        "Y = lyric_df['sentiment_bin'].values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state=123)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCHdzrLZSFHk",
        "outputId": "a1f638e2-e1ee-4101-d1dd-1435a9c25910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "16/16 [==============================] - 3s 61ms/step - loss: 0.6761 - accuracy: 0.7000\n",
            "Epoch 2/6\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.6037 - accuracy: 0.7375\n",
            "Epoch 3/6\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.4831 - accuracy: 0.7375\n",
            "Epoch 4/6\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.3923 - accuracy: 0.7812\n",
            "Epoch 5/6\n",
            "16/16 [==============================] - 1s 63ms/step - loss: 0.2634 - accuracy: 0.8750\n",
            "Epoch 6/6\n",
            "16/16 [==============================] - 1s 64ms/step - loss: 0.1753 - accuracy: 0.9438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f66de0a4e90>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "batch_size = 10\n",
        "model.fit(X_train,\n",
        "          Y_train,\n",
        "          epochs=6, batch_size = batch_size,\n",
        "          verbose = 'auto'\n",
        "           )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting LSTM model with Real Data (Training and Testing Accuracy)"
      ],
      "metadata": {
        "id": "RqwCWixEJyzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i31nMSQS62L"
      },
      "outputs": [],
      "source": [
        "predictions_nn_train = model.predict(X_train)\n",
        "predictions_nn_test = model.predict(X_test)\n",
        "for i in range(len(predictions_nn_train)):\n",
        "    if predictions_nn_train[i][0] < 0.5:\n",
        "        predictions_nn_train[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_train[i][0] = 1\n",
        "        \n",
        "for i in range(len(predictions_nn_test)):\n",
        "    if predictions_nn_test[i][0] < 0.5:\n",
        "        predictions_nn_test[i][0] = 0\n",
        "    else:\n",
        "        predictions_nn_test[i][0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-wm9gSbS9B_",
        "outputId": "396168cd-5842-4fb6-ac8c-5755b711cba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.9875\n",
            "Test accuracy 0.875\n"
          ]
        }
      ],
      "source": [
        "print('Train accuracy:', accuracy_score(Y_train, predictions_nn_train))\n",
        "print('Test accuracy', accuracy_score(Y_test, predictions_nn_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting LSTM model with Synthetic Data"
      ],
      "metadata": {
        "id": "REtNZSuqJpT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_word_count_syn = syn_data['lyrics'].str.split().apply(len)\n",
        "max_features_syn = max(total_word_count_syn)\n",
        "max_features_syn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PEhWWyIIoyV",
        "outputId": "ab760a2a-1da6-4e1c-d0ba-da09a15fb983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "317"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_syn = Tokenizer(num_words=max_features_syn, split=' ')\n",
        "tokenizer_syn.fit_on_texts(syn_data['lyrics'].values)\n",
        "X_syn = tokenizer.texts_to_sequences(syn_data['lyrics'].values)\n",
        "X_syn = pad_sequences(X_syn)"
      ],
      "metadata": {
        "id": "J_v2N3mnITL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn_data['sentiment_bin'] = syn_data['sentiment'].apply(lambda x: 1 if (x =='pos') else 0) \n",
        "Y_syn = syn_data['sentiment_bin'].values"
      ],
      "metadata": {
        "id": "5sb7NrF3I9e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### LSTM with synthetic \n",
        "predictions_syn = model.predict(X_syn)\n",
        "for i in range(len(predictions_syn)):\n",
        "    if predictions_syn[i][0] < 0.5:\n",
        "        predictions_syn[i][0] = 0\n",
        "    else:\n",
        "        predictions_syn[i][0] = 1\n"
      ],
      "metadata": {
        "id": "LeqqnOQSIDmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Synthetic data accuracy:', accuracy_score(Y_syn, predictions_syn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlL6azw-JZw3",
        "outputId": "68384165-e6a2-4fbb-df58-3759caadb47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synethetic data accuracy: 0.77\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "703 Final proj.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}